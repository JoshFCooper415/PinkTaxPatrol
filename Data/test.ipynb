{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshf\\AppData\\Local\\Temp\\ipykernel_12536\\810541465.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "C:\\Users\\joshf\\AppData\\Local\\Temp\\ipykernel_12536\\810541465.py:41: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Keyword'] = df['Keyword'].replace({'men': 0, 'women': 1, 'missing': -1})  # Assuming 'missing' as -1 or another category\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib  # for saving and loading sklearn models\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Function to apply all transformations\n",
    "def preprocess_data(df, vectorizer=None, fit_vectorizer=True):\n",
    "    \"\"\"\n",
    "    Applies all transformations to the dataframe `df`.\n",
    "    \n",
    "    Args:\n",
    "    - df: pandas DataFrame, input data to transform.\n",
    "    - vectorizer: CountVectorizer instance, if None a new one will be created and fit.\n",
    "    - fit_vectorizer: bool, whether to fit the CountVectorizer or not (useful at inference).\n",
    "    \n",
    "    Returns:\n",
    "    - df: transformed pandas DataFrame.\n",
    "    - vectorizer: fitted CountVectorizer instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fit and transform the product names\n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "    \n",
    "    if fit_vectorizer:\n",
    "        X = vectorizer.fit_transform(df['Product Name']).toarray()\n",
    "    else:\n",
    "        X = vectorizer.transform(df['Product Name']).toarray()\n",
    "    \n",
    "    # Create a DataFrame with the one-hot encoded features\n",
    "    df_one_hot = pd.DataFrame(X, columns=vectorizer.get_feature_names_out(), index=df.index)\n",
    "    #df = df.drop(['Product Name'])\n",
    "    # Concatenate the original DataFrame with the new one-hot encoded DataFrame\n",
    "    df = pd.concat([df, df_one_hot], axis=1)\n",
    "    df.fillna(0, inplace=True)\n",
    "    df['Unit Count'] = df['Unit Count'].replace(0, 1)  # Replace 0 units with 1 to avoid division by zero\n",
    "    df['true_price'] = df['Price'] / df['Unit Count']\n",
    "    df['Keyword'] = df['Keyword'].replace({'men': 0, 'women': 1, 'missing': -1})  # Assuming 'missing' as -1 or another category\n",
    "    \n",
    "    # One-hot encode 'Category'\n",
    "    one_hot_encoded = pd.get_dummies(df['Category'], prefix='Category')\n",
    "    df = df.join(one_hot_encoded)\n",
    "    \n",
    "    # Replace spaces with underscores in column names and sort columns\n",
    "    df.columns = [col.replace(' ', '_') for col in df.columns]\n",
    "    df = df.sort_index(axis=1)\n",
    "    \n",
    "    return df, vectorizer\n",
    "\n",
    "df = pd.read_csv('amazon_products_via_rainforest_api.csv')\n",
    "df2 = pd.read_csv('amazon_products_via_rainforest_api2.csv')\n",
    "df3 = pd.read_csv('amazon_products_via_rainforest_api3.csv')\n",
    "df4 = pd.read_csv('amazon_products_via_rainforest_api4.csv')\n",
    "df5 = pd.read_csv('amazon_products_via_rainforest_api5.csv')\n",
    "df6 = pd.read_csv('amazon_products_via_rainforest_api6.csv')\n",
    "df7 = pd.read_csv('amazon_products_via_rainforest_api7.csv')\n",
    "df5['Price'] = df5['Price'].str.replace('$', '').astype(float)\n",
    "df6['Price'] = df6['Price'].str.replace('$', '').astype(float)\n",
    "df7['Price'] = df7['Price'].str.replace('$', '').astype(float)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df = pd.concat([df,df2,df3,df4,df5,df6,df7], axis=0)\n",
    "\n",
    "# Example usage during training\n",
    "df, vectorizer = preprocess_data(df)\n",
    "df = df.drop(['Product_Name','ASIN','Category'], axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can only compare identically-labeled Series objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m df_filtered \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(index\u001b[38;5;241m=\u001b[39moriginal_index)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Filter rows with lower 'true_price' and sort by similarity\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m similar_rows \u001b[38;5;241m=\u001b[39m df_filtered[\u001b[43mdf_filtered\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue_price\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43moriginal_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrue_price\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m]\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Select the top 3 most similar rows\u001b[39;00m\n\u001b[0;32m     21\u001b[0m top_3_similar \u001b[38;5;241m=\u001b[39m similar_rows\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:48\u001b[0m, in \u001b[0;36mOpsMixin.__lt__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__lt__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__lt__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:6094\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6091\u001b[0m res_name \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_op_result_name(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[0;32m   6093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, Series) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indexed_same(other):\n\u001b[1;32m-> 6094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan only compare identically-labeled Series objects\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6096\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   6097\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: Can only compare identically-labeled Series objects"
     ]
    }
   ],
   "source": [
    "\n",
    "original_index = 0\n",
    "\n",
    "# Preprocessing: Scale features (excluding 'true_price') for cosine similarity\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cos_sim_matrix = cosine_similarity(scaled_features)\n",
    "\n",
    "# Extract similarity scores for the original row and create a DataFrame for them\n",
    "similarity_scores = pd.Series(cos_sim_matrix[original_index], index=df.index)\n",
    "\n",
    "# Add similarity scores to the original DataFrame (excluding the original row itself)\n",
    "df['similarity'] = similarity_scores\n",
    "df_filtered = df.drop(index=original_index)\n",
    "\n",
    "# Filter rows with lower 'true_price' and sort by similarity\n",
    "similar_rows = df_filtered[df_filtered['true_price'] < df.loc[original_index, 'true_price']].sort_values(by='similarity', ascending=False)\n",
    "\n",
    "# Select the top 3 most similar rows\n",
    "top_3_similar = similar_rows.head(3)\n",
    "\n",
    "print(top_3_similar)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
