{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshf\\AppData\\Local\\Temp\\ipykernel_25068\\3164799568.py:42: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['Keyword'] = df['Keyword'].replace({'men': 0, 'women': 1, 'missing': -1})  # Assuming 'missing' as -1 or another category\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import joblib  # for saving and loading sklearn models\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "# Function to apply all transformations\n",
    "def preprocess_data(df, vectorizer=None, fit_vectorizer=True):\n",
    "    \"\"\"\n",
    "    Applies all transformations to the dataframe `df`.\n",
    "    \n",
    "    Args:\n",
    "    - df: pandas DataFrame, input data to transform.\n",
    "    - vectorizer: CountVectorizer instance, if None a new one will be created and fit.\n",
    "    - fit_vectorizer: bool, whether to fit the CountVectorizer or not (useful at inference).\n",
    "    \n",
    "    Returns:\n",
    "    - df: transformed pandas DataFrame.\n",
    "    - vectorizer: fitted CountVectorizer instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fit and transform the product names\n",
    "    if vectorizer is None:\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "    \n",
    "    if fit_vectorizer:\n",
    "        X = vectorizer.fit_transform(df['Product Name']).toarray()\n",
    "    else:\n",
    "        X = vectorizer.transform(df['Product Name']).toarray()\n",
    "    \n",
    "    # Create a DataFrame with the one-hot encoded features\n",
    "    df_one_hot = pd.DataFrame(X, columns=vectorizer.get_feature_names_out(), index=df.index)\n",
    "    #df = df.drop(['Product Name'])\n",
    "    # Concatenate the original DataFrame with the new one-hot encoded DataFrame\n",
    "    df = pd.concat([df, df_one_hot], axis=1)\n",
    "    df.fillna(0, inplace=True)\n",
    "    df['Unit Count'] = df['Unit Count'].replace(0, 1)  # Replace 0 units with 1 to avoid division by zero\n",
    "    df['true_price'] = df['Price'] / df['Unit Count']\n",
    "    df['Keyword'] = df['Keyword'].replace({'men': 0, 'women': 1, 'missing': -1})  # Assuming 'missing' as -1 or another category\n",
    "    \n",
    "    # One-hot encode 'Category'\n",
    "    one_hot_encoded = pd.get_dummies(df['Category'], prefix='Category')\n",
    "    df = df.join(one_hot_encoded)\n",
    "    \n",
    "    # Replace spaces with underscores in column names and sort columns\n",
    "    df.columns = [col.replace(' ', '_') for col in df.columns]\n",
    "    df = df.sort_index(axis=1)\n",
    "    \n",
    "    return df, vectorizer\n",
    "\n",
    "df = pd.read_csv('amazon_products_via_rainforest_api.csv')\n",
    "df2 = pd.read_csv('amazon_products_via_rainforest_api2.csv')\n",
    "df3 = pd.read_csv('amazon_products_via_rainforest_api3.csv')\n",
    "df4 = pd.read_csv('amazon_products_via_rainforest_api4.csv')\n",
    "df5 = pd.read_csv('amazon_products_via_rainforest_api5.csv')\n",
    "df6 = pd.read_csv('amazon_products_via_rainforest_api6.csv')\n",
    "df7 = pd.read_csv('amazon_products_via_rainforest_api7.csv')\n",
    "df5['Price'] = df5['Price'].str.replace('$', '').astype(float)\n",
    "df6['Price'] = df6['Price'].str.replace('$', '').astype(float)\n",
    "df7['Price'] = df7['Price'].str.replace('$', '').astype(float)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df = pd.concat([df,df2,df3,df4,df5,df6,df7], axis=0)\n",
    "# Example usage during training\n",
    "df, vectorizer = preprocess_data(df)\n",
    "df = df.drop_duplicates(subset=['ASIN'])\n",
    "asinDf = df['ASIN']\n",
    "df = df.drop(['Product_Name','ASIN','Category'], axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find indices of rows where 'Keyword' equals 1\n",
    "indices_to_drop = df[df['Keyword'] == 1].index\n",
    "\n",
    "# Drop these rows\n",
    "df = df.drop(index=indices_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     002  004  01  024  04  04936  05fl  07  0fl  10  ...  yrl  ysl  yves  \\\n",
      "5      0    0   0    0   0      0     0   0    0   1  ...    0    0     0   \n",
      "155    0    0   0    0   0      0     0   0    0   0  ...    0    0     0   \n",
      "137    0    0   0    0   0      0     0   0    0   0  ...    0    0     0   \n",
      "\n",
      "     zaru  zero  zest  zinc  zipper  zone  similarity  \n",
      "5       0     0     0     0       0     0    0.101139  \n",
      "155     0     0     0     0       0     0    0.054132  \n",
      "137     0     0     0     0       0     0    0.051114  \n",
      "\n",
      "[3 rows x 3117 columns]\n"
     ]
    }
   ],
   "source": [
    "original_index = 0\n",
    "\n",
    "# Preprocessing: Scale features (excluding 'true_price') for cosine similarity\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(df)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cos_sim_matrix = cosine_similarity(scaled_features)\n",
    "\n",
    "# Extract similarity scores for the original row and create a DataFrame for them\n",
    "similarity_scores = pd.Series(cos_sim_matrix[original_index], index=df.index)\n",
    "\n",
    "# Add similarity scores to the original DataFrame (excluding the original row itself)\n",
    "df['similarity'] = similarity_scores\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df_filtered = df.drop(index=original_index)\n",
    "\n",
    "\n",
    "# Filter rows with lower 'true_price' and sort by similarity\n",
    "similar_rows = df_filtered[df_filtered['true_price'] < df.loc[original_index, 'true_price']].sort_values(by='similarity', ascending=False)\n",
    "\n",
    "# Select the top 3 most similar rows\n",
    "top_3_similar = similar_rows.head(3)\n",
    "\n",
    "print(top_3_similar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B0CRYF8SYP\n",
      "B0C3R8P5YJ\n",
      "B00SD7DCLQ\n"
     ]
    }
   ],
   "source": [
    "print(asinDf.iloc[top_3_similar.index[0]])\n",
    "print(asinDf.iloc[top_3_similar.index[1]])\n",
    "print(asinDf.iloc[top_3_similar.index[2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([5, 155, 137], dtype='int64')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_3_similar.index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
